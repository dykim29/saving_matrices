{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook explores different ways we can save the large similarity matrix in a more compressed format.\n",
    "\n",
    "Similarity matrix is dense (all values filled in), and symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import s3fs\n",
    "import sys\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import seaborn as sns\n",
    "\n",
    "from datascience.menupackage import data_loader\n",
    "from datascience.menupackage import data_retrieval\n",
    "from datascience.connections import redshift\n",
    "from datascience import queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 's3-gousto-artichokes-airflow'\n",
    "s3_key = 'static_inputs/similarity_results.npz'\n",
    "s3_connection = s3fs.S3FileSystem(anon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 599 ms, sys: 566 ms, total: 1.16 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "similarity_matrix = data_loader.Recipe_Item_Loader.retrieve_similarity_matrix(\n",
    "    s3_path=os.path.join(bucket, s3_key),\n",
    "    fs=s3_connection\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First method: original method using np.savez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_npz(dataframe: pd.DataFrame) -> None:\n",
    "    np.savez('original_method.npz', arr_0=dataframe.values, arr_1=dataframe.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.6 ms, sys: 42.2 ms, total: 65.8 ms\n",
      "Wall time: 66.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_npz(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original file size is 88.951626 MB\n"
     ]
    }
   ],
   "source": [
    "print('Original file size is {} MB'.format(os.path.getsize('original_method.npz')/10**6))\n",
    "original_size = os.path.getsize('original_method.npz')/10**6\n",
    "#os.remove('original_method.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 ms, sys: 24.9 ms, total: 51.1 ms\n",
      "Wall time: 49.8 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim = np.load('original_method.npz')\n",
    "similarity_matrix2 = pd.DataFrame(sim['arr_0'], columns=sim['arr_1'])\n",
    "similarity_matrix2.index = sim['arr_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second method: simply use np.savez_compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New file size is ...\n",
    "def write_npz_compressed(dataframe: pd.DataFrame) -> None:\n",
    "    np.savez_compressed('matrix.npz', arr_0=dataframe.values, arr_1=dataframe.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.74 s, sys: 30.2 ms, total: 2.77 s\n",
      "Wall time: 2.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "write_npz_compressed(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 200 ms, sys: 15.3 ms, total: 215 ms\n",
      "Wall time: 217 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim = np.load('matrix.npz')\n",
    "similarity_matrix2 = pd.DataFrame(sim['arr_0'], columns=sim['arr_1'])\n",
    "similarity_matrix2.index = sim['arr_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed file size is 14.138965 MB\n",
      "This is 15.90% of original size\n"
     ]
    }
   ],
   "source": [
    "print('Compressed file size is {} MB'.format(os.path.getsize('matrix.npz')/10**6))\n",
    "compressed_size = os.path.getsize('matrix.npz')/10**6\n",
    "print(\"This is {:.2f}% of original size\".format(os.path.getsize('matrix.npz')/os.path.getsize('original_method.npz')*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size is smaller by 0.15895116970655487\n"
     ]
    }
   ],
   "source": [
    "print(\"Size is smaller by {}\".format(compressed_size/original_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File is only 16% of its original size, but the time taken to save is a bit longer. But in reality the limiting factor to writing to s3 is probably caused by transfer of the actual data over the internet (i.e. the bottleneck is not the CPU), time taken to save is still only 200ms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third method: make use of symmetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could save only the upper half triangle of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = similarity_matrix.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_npz_compressed_upper_half(dataframe: pd.DataFrame) -> None:\n",
    "    length = len(data)\n",
    "    np.savez_compressed('third_method.npz', arr_0=dataframe.values[np.triu_indices(length, 1)], arr_1=dataframe.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.47 s, sys: 69.6 ms, total: 1.54 s\n",
      "Wall time: 1.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_npz_compressed_upper_half(similarity_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size is 7.260964 MB\n",
      "This is 8.16% of original size\n"
     ]
    }
   ],
   "source": [
    "print('file size is {} MB'.format(os.path.getsize('third_method.npz')/10**6))\n",
    "print(\"This is {:.2f}% of original size\".format(os.path.getsize('third_method.npz')/os.path.getsize('original_method.npz')*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have to add some logic to loading the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.53 s, sys: 316 ms, total: 7.84 s\n",
      "Wall time: 8.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sim = np.load('third_method.npz')\n",
    "saved_values = sim['arr_0']\n",
    "cols = sim['arr_1']\n",
    "\n",
    "sim_matrix = np.zeros((len(col), len(col)))\n",
    "for i, (j, k) in enumerate(zip(np.triu_indices(len(cols), 1)[0], np.triu_indices(len(cols), 1)[1])):\n",
    "    sim_matrix[j][k] = saved[i]\n",
    "    sim_matrix[k][j] = saved[i]\n",
    "reconstructed_sim_matrix = pd.DataFrame(sim_matrix, columns=cols, index=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken to 'unpack' the values takes some time - it would have O(N^2) time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this file was saved in S3, how long would it take?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. original method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 696 ms, sys: 637 ms, total: 1.33 s\n",
      "Wall time: 15.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sims = data_retrieval.Recipe_Item_Retrieval.retrieve_similarities(\n",
    "    s3_path=os.path.join('s3-gousto-artichokes-airflow' ,'static_inputs/similarity_results.npz'),\n",
    "    fs=s3_connection\n",
    ")\n",
    "similarity_matrix = pd.DataFrame(sims['arr_0'],columns = sims['arr_1'])\n",
    "similarity_matrix.index = sims['arr_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.95 s, sys: 170 ms, total: 7.12 s\n",
      "Wall time: 7.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sims = data_retrieval.Recipe_Item_Retrieval.retrieve_similarities(\n",
    "    s3_path=os.path.join('s3-gousto-artichokes-airflow' ,'static_inputs/third_method.npz'),\n",
    "    fs=s3_connection\n",
    ")\n",
    "saved_values = sims['arr_0']\n",
    "cols = sims['arr_1']\n",
    "\n",
    "sim_matrix = np.zeros((len(col), len(col)))\n",
    "for i, (j, k) in enumerate(zip(np.triu_indices(len(cols), 1)[0], np.triu_indices(len(cols), 1)[1])):\n",
    "    sim_matrix[j][k] = saved[i]\n",
    "    sim_matrix[k][j] = saved[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last try - use Hdf5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_1\": shape (3334, 3334), type \"<f8\">"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "a = similarity_matrix.values\n",
    "h5f = h5py.File('data.h5', 'w')\n",
    "h5f.create_dataset('dataset_1', data=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file size is 88.926496 MB\n"
     ]
    }
   ],
   "source": [
    "print('file size is {} MB'.format(os.path.getsize('data.h5')/10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Never mind this doesn't reduce anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- with no changes, loading the data from S3, then unpacking the numpy array into a dataframe took wall time of 13.7 s on my local machine and CPU time of 616ms. In fact most time taken came from converting the 2d numpy array into a dataframe.\n",
    "- size wise, it is best to use `np.savez_compressed` to use the compressed format, which reduces the size automatically by 6.25 times. This has the advantage that the method of loading of the matrix does not change at all so we won't have to change any of the code which ingests the similarity matrix. The time taken to save the matrix takes longer (23ms to 2.7s) but in pratice, the overhead for uploading would probably come from transfer of data into S3 rather than the compression.\n",
    "- to halve that, we can save just the upper half of the triangle, which would make the size of file 12.3 times smaller than original. In fact, unpacking the array into a dataframe actually takes shorter time (wall time wise, it decreases by half, CPU time it increases a lot, not sure why? It's not waiting for a network or anything) from this, compared to the original method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up files\n",
    "files = ['matrix.npz', 'data.h5', 'original_method.npz', 'third_method.npz']\n",
    "for file in files:\n",
    "    os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
